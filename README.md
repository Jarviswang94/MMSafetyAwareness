<h1 align="center">Multimodal Safety Awareness Benchmark for MLLMs</h1>
Can't See the Forest for the Trees: Benchmarking  Multimodal Safety Awareness for Multimodal LLMs
<br>   <br>

<div align="center">
  <img src="paper/Overview.png" width="500">
</div>

<h3 align="center">RESEARCH USE ONLY✅ NO MISUSE❌</h3>



## 💡Dataset
<div align="center">
  <img src="paper/data1.pdf" alt="Logo" width="500">
</div>


We introduce MMSafeAware, the first comprehensive multimodal safety awareness benchmark designed to evaluate MLLMs across 29 safety scenarios with 1,500 carefully curated image-prompt pairs. MMSafeAware includes both unsafe and over-safety subsets to assess models' abilities to correctly identify unsafe content and avoid over-sensitivity that can hinder helpfulness. Evaluating nine widely used MLLMs using MMSafeAware reveals that current models are not sufficiently safe and often overly sensitive; for example, GPT-4V misclassifies 36.1% of unsafe inputs as safe and 59.9% of benign inputs as unsafe. 
  

## 📃Results

<div align="center">
  <img src="paper/result.jpg" alt="Logo" width="500">
</div>





## 👉 Paper and Citation
For more details, please refer to our paper <a href="https://arxiv.org/abs/2502.11184">here</a>.



## Citation

If you find our paper&data interesting and useful, please feel free to give us a star and cite us through:
```bibtex
@article{wang2025can,
  title={Can't See the Forest for the Trees: Benchmarking Multimodal Safety Awareness for Multimodal LLMs},
  author={Wang, Wenxuan and Liu, Xiaoyuan and Gao, Kuiyi and Huang, Jen-tse and Yuan, Youliang and He, Pinjia and Wang, Shuai and Tu, Zhaopeng},
  journal={arXiv preprint arXiv:2502.11184},
  year={2025}
}
```

